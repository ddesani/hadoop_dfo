/*
 * dfo_hadoop.c
 *
 * Otimiza??o sem o uso de derivadas (DFO) dos par?metros de simula??o de Map-
 * Reduce. A otimiza??o ? feita pelo NLopt, com o algoritmo COBYLA
 * (http://ab-initio.mit.edu/wiki/index.php/NLopt_Introduction).
 * A simula??o de Map-Reduce ? feita pelo MRSG, que atua sobre o SimGrid.
 *
 *
 * Autores:
 *					Daniel Mueller	<dsmueller.mueller@gmail.com>
 *	Convertido para C com NLopt por Samuel Silva	<samuelrsilva@gmail.com>
 *	Versao DFO + Hadoop por		Diego Desani	<ddesani@gmail.com>
 *
 * Vari?veis do problema e respectivas posi??es em x[]:
 *   0 - mapred.child.java.opts (int)
 *
 *
 *
 * As M restri??es n?o lineares (nonlinear constraints) s?o do tipo:
 *       c_l <= fc_i(x) <= c_u, para fun??es de restri??o fc_i(x) e i = 1,...,M
 *
 * Como NLopt usa apenas restri??es n?o lineares do tipo fc_i(x) <= 0, as
 * restri??es usadas neste programa s?o:
 * fc_i'(x) = -fc_i(x) + c_l <= 0        e       fc_i''(x) = fc_i(x) - c_u  <= 0
 *
 *
 * A restri??o implementada ?
 * 1- (ChunkSize)*(# of maps) = const (entre 499999 e 500001) ** DELETAR **
 * 1- mapred.child.java.opts <= 80% RAM
 *
 * Est?o planejadas as restri??es adicionais:
 * 2- (Chunk size)/(Map cost) = const
 * 3- (reduce cost)/(map Out) = const
 * 4- (# of maps)/(# of reduces) = const
 */

#define _GNU_SOURCE

#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/time.h>
#include <pthread.h>
#include <string.h>
#include "nlopt.h"

// #define _POSIX_C_SOURCE >= 199309L
#define MAXLEN	255            /* comprimento maximo de strings */
#define REPETICOES	2
#define toseconds(t) (t.tv_sec + (t.tv_nsec/1000000000.))

/* parametros variaveis */
enum {
	MAPRED_CHILD_JAVA_OPTS = 0,
	IO_SORT_MB,
	IO_SORT_FACTOR,
	IO_SORT_RECORD_PERCENT,
	IO_FILE_BUFFER_SIZE,
	MAPRED_JOB_SHUFFLE_INPUT_BUFFER_PERCENT,
	MAPRED_JOB_SHUFFLE_MERGE_PERCENT,
	MAPRED_INMEM_MERGE_THRESHOLD,
	MAPRED_JOB_REDUCE_INPUT_BUFFER_PERCENT,
	N    
};

/* parametros fixos */
enum {
        FS_DEFAULT_NAME = 0,
        DFS_REPLICATION,
        DFS_NAME_DIR,
        DFS_DATA_DIR,
        MAPRED_JOB_TRACKER,
        MAPRED_LOCAL_DIR,
	MAPRED_MAP_TASKS,
        N_DEFAULT_PARAMS
};

/* nome para parametros variaveis */
const char *x_names[N] = {
	"mapred.child.java.opts",
	"io.sort.mb",
	"io.sort.factor",
	"io.sort.record.percent",
	"io.file.buffer.size",
	"mapred.job.shuffle.input.buffer.percent",
	"mapred.job.shuffle.merge.percent",
	"mapred.inmem.merge.threshold",
	"mapred.job.reduce.input.buffer.percent"
};

/* nome para parametros fixos */
const char *x_default_params[N_DEFAULT_PARAMS] = {
	"fs.default.name",
	"dfs.replication",
	"dfs.name.dir",
	"dfs.data.dir",
	"mapred.job.tracker",
	"mapred.local.dir",
	"mapred.map.tasks"
};

/* Current working nodes */
const int nodes[] = {10, 11, 13, 14, 15, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 40, 41, 42, 43}; 
int nodes_size = sizeof(nodes) / sizeof(int);


int evals;  /* avalia??es da fun??o objetivo */

/*
 * Calcular as restri??es c_l <= fc_i(x) <= c_u, separando em duas etapas:
 *    inferior                                        superior
 * fc_i'(x) <= 0                    e          fc_i''(x) <= 0
 * fc_i'(x) = -fc_i(x) + c_l        e          fc_i''(x) = fc_i(x) - c_u
 *
 */


 /* Restricao io.sort.mb */
double restricao_io_sort_mb(unsigned n, const double *x, double *grad, void *data)
{
	double *c_l = (double *) data;

	return x[IO_SORT_MB] * 100 / x[MAPRED_CHILD_JAVA_OPTS] - *c_l;
}

 /* Restricao io.sort.factor */
double restricao_io_sort_factor(unsigned n, const double *x, double *grad, void *data)
{
        double *c_l = (double *) data;
        
        return x[IO_SORT_FACTOR] * 100 / x[IO_SORT_MB] - *c_l;
}

void *updateNodes(void *threadid)
{
	char buffer[MAXLEN];
	int *id_ptr, node;

	id_ptr = (int *) threadid;
        node = *id_ptr;

        /* Run chef-client */
        snprintf(buffer, MAXLEN,"knife ssh 'name:node-%d' 'chef-client'", node);
        system(buffer);
}


/**********************/
inline void configurar_mrsg(const double *x)
{
	char buffer[MAXLEN];
	pthread_t threads[nodes_size];
	int *taskids[nodes_size];
	int ret = 0;
	int i = 0;
	FILE *f;

	/* Escrita do arquivo hadoop_parameters.conf */
	f = fopen("/opt/hadoop_parameters.conf", "w");
	fprintf(f, 
		"## Hadoop - parameters file\n"
		"# Generated by dfo_mapreduce.c\n"
		"id = properties\n"
		"\n"
	);
	
	
	/* CORE session */
	fprintf(f,
		"[core]\n"
		"%s = hdfs://node-10:54310\n"
		"\n",
		x_default_params[FS_DEFAULT_NAME]
	);
	
	/* HDFS session */
	fprintf(f,
		"[hdfs]\n"
		"%s = 1\n"
		"%s = /disk-1/nn\n"
		"%s = /disk-2/dn\n"
		"\n",
		x_default_params[DFS_REPLICATION],
		x_default_params[DFS_NAME_DIR],
		x_default_params[DFS_DATA_DIR]
	);
	
	/* MAPRED session */
	fprintf(f,
		"[mapred]\n"
		"%s = node-10:54311\n"
		"%s = /mnt/mapred/jt\n"
		"%s = 56\n"
		/* Parametros DFO */
		"%s = -Xmx%ldm\n" //	mapred.child.java.opts
		"%s = %ld\n"	//	io.sort.mb
		"%s = %d\n"	//	io.sort.factor
		"%s = %.2f\n"	//	io.sort.record.percent
		"%s = %ld\n"	//	io.file.buffer.size
		"%s = %.2f\n"	//	mapred.job.shuffle.input.buffer.percent
		"%s = %.2f\n"	//	mapred.job.shuffle.merge.percent
		"%s = %d\n"	//	mapred.inmem.merge.threshold
		"%s = %.2f\n"	//	mapred.job.reduce.input.buffer.percent
		"\n",
		x_default_params[MAPRED_JOB_TRACKER],
		x_default_params[MAPRED_LOCAL_DIR],
		x_default_params[MAPRED_MAP_TASKS],
		/* Parametros DFO */
		x_names[MAPRED_CHILD_JAVA_OPTS], (long)x[MAPRED_CHILD_JAVA_OPTS],
		x_names[IO_SORT_MB], (long)x[IO_SORT_MB],
		x_names[IO_SORT_FACTOR], (int)x[IO_SORT_FACTOR],
		x_names[IO_SORT_RECORD_PERCENT], (float)x[IO_SORT_RECORD_PERCENT],
		x_names[IO_FILE_BUFFER_SIZE], (long)x[IO_FILE_BUFFER_SIZE],
		x_names[MAPRED_JOB_SHUFFLE_INPUT_BUFFER_PERCENT], (float)x[MAPRED_JOB_SHUFFLE_INPUT_BUFFER_PERCENT],
		x_names[MAPRED_JOB_SHUFFLE_MERGE_PERCENT], (float)x[MAPRED_JOB_SHUFFLE_MERGE_PERCENT],
		x_names[MAPRED_INMEM_MERGE_THRESHOLD], (int)x[MAPRED_INMEM_MERGE_THRESHOLD],
		x_names[MAPRED_JOB_REDUCE_INPUT_BUFFER_PERCENT], (float)x[MAPRED_JOB_REDUCE_INPUT_BUFFER_PERCENT]
	);

	fclose(f);
	/* Stop Hadoop Services */
	snprintf(buffer, MAXLEN,"ssh node-10 '/etc/init.d/hadoop stop'");
        system(buffer);

	/* update data bag JSON */
	snprintf(buffer, MAXLEN,"ruby /opt/conf_to_json.rb");
        system(buffer);

	/* upload data bag */
	snprintf(buffer, MAXLEN,"knife data bag from file hadoopdfo /data/chef/chef-repo/data_bags/hadoopdfo/hadoop_properties.json");
        system(buffer);

	/* Run chef-client */
	// `knife client list | grep node`
	for( i = 0; i < nodes_size; ++i) {
		taskids[i] = (int *) malloc(sizeof(int));
		*taskids[i] = nodes[i];
		printf("Creating thread %d\n", i);
		ret = pthread_create(&threads[i], NULL, updateNodes, (void *) taskids[i]);
		if (ret) {
			printf("ERROR; return code from pthread_create() is %d\n", ret);
			exit(-1);
		}
	}
	for ( i = 0; i < nodes_size; ++i)
		pthread_join(threads[i], NULL);

	// snprintf(buffer, MAXLEN," for i in `knife client list | grep node`; do knife ssh \"name:$i\" 'chef-client'; done ");
        // system(buffer);
	
	
	/* Start Hadoop Services */
	snprintf(buffer, MAXLEN,"ssh node-10 '/etc/init.d/hadoop start'; sleep 150");
        system(buffer);
}

/* Disparar as simula??es sequencial e paralela e retornar a efici?ncia */
inline double executar(const double *x)
{
	char buffer[MAXLEN], *job_status;
	double time;
	FILE *f, *job_output_status;


	/* Limpar qualquer conteudo no output  */
	snprintf(buffer, MAXLEN,
		"ssh hadoop@node-10 \"hadoop dfs -rmr indices/* \" ");
	system(buffer);
	
	/* Limpar arquivos temporarios  */
	snprintf(buffer, MAXLEN,
		"ssh hadoop@node-10 \"/mnt/clean_hdfs_tmp.sh \" ");
	system(buffer); 

	/* Executar Job */
	snprintf(buffer, MAXLEN,
		"ssh hadoop@node-10  \"/mnt/terrier-3.5/bin/trec_terrier.sh -i -H \" ");
	system(buffer);


	/* Verificar se execucao foi OK */
	job_output_status = popen("ssh hadoop@10.22.0.10 'hadoop job -history all indices' | grep Status| awk '{print $2}'", "r");
	job_status = fgets(buffer, sizeof(buffer), job_output_status);
	if ( strcmp("FAILED\n",job_status) == 0 )
		time = 10000;
	else{
		/* Salvar tempo */
		snprintf(buffer, MAXLEN,
        	        "ssh hadoop@node-10  \"hadoop job -history all indices | egrep 'Launched At|Finished At'\" | awk '{system(\"date -d\\\"\"$3\" \"$4 \"\\\" +%%s\")}' | awk 'BEGIN{b=0}{b=$1 - b} END {print b}' > result.txt ");
	        system(buffer);
	
		f = fopen("result.txt", "r");
		if (f != NULL) {
			if (!fgets(buffer, MAXLEN, f))
				fprintf(stderr, "Tempo nao lido\n");
			time = atof(buffer);
			fclose(f);
		} else {
			perror("fopen: result.txt");
			time = 0;
		}
	}
	return time;
}

/* Algumas vari?veis s?o definidas como n?meros inteiros, ent?o ? preciso
 * arredond?-las */
inline void arredondar_valores(double *x)
{
	x[MAPRED_CHILD_JAVA_OPTS] = round(x[MAPRED_CHILD_JAVA_OPTS]);
	x[IO_SORT_MB] = round(x[IO_SORT_MB]);
	x[IO_SORT_FACTOR] = round(x[IO_SORT_FACTOR]);
	x[IO_FILE_BUFFER_SIZE] = round(x[IO_FILE_BUFFER_SIZE]);
	x[MAPRED_INMEM_MERGE_THRESHOLD] = round(x[MAPRED_INMEM_MERGE_THRESHOLD]);
}

/*
 * Fun??o cujo valor ser? maximizado.
 */
double objetivo(unsigned n, double *x, double *grad, void *my_func_data)
{
	double f = 0;
	int j, i = 0;

	++evals;

	arredondar_valores(x);
	configurar_mrsg(x);
	for ( i = 0; i < REPETICOES; ++i ){
		f += executar(x); /* tempo total de job */
		printf("PARCIAL: %d, f = %g\n", evals, f);
		for (j = 0; j < N; ++j)
                	printf("PARCIAL: %15g   %s\n", x[j], x_names[j]);
	}
	f = -f / REPETICOES;
	
	printf("%d, f = %g\n", evals, f);

	for (i = 0; i < N; i++)
		printf("%15g   %s\n", x[i], x_names[i]);
	fflush(stdout);

	return f;
}

/*
 * Ler os dados de entrada a partir de stdin.
 * Veja arquivo de entrada fornecido.
 */
void ler_entrada(double x[N], double lb[N], double ub[N],
		double c1data[2], double *stopval, double *tol, int *maxeval)
{
	char buffer[MAXLEN];
	char *nao_fim;
	int i;

	i = 0;
	nao_fim = fgets(buffer, MAXLEN, stdin);
	while (i < N && nao_fim) {

		if (buffer[0] != '#') {
			sscanf(buffer, "%lf %lf %lf", &x[i], &lb[i], &ub[i]);
			i++;
		}
		nao_fim = fgets(buffer, MAXLEN, stdin);
	}

	while (nao_fim && buffer[0] == '#')
		nao_fim = fgets(buffer, MAXLEN, stdin);

	/* io.sort.mb - c_l and c_u */
	sscanf(buffer, "%lf", &c1data[0]); 
	
	nao_fim = fgets(buffer, MAXLEN, stdin);
        while (nao_fim && buffer[0] == '#')
                nao_fim = fgets(buffer, MAXLEN, stdin);
	
	/* io.sort.factor - c_l and c_u */	
	sscanf(buffer, "%lf", &c1data[1]); 

	nao_fim = fgets(buffer, MAXLEN, stdin);
	while (nao_fim && buffer[0] == '#')
		nao_fim = fgets(buffer, MAXLEN, stdin);

	sscanf(buffer, "%lf", stopval);

	nao_fim = fgets(buffer, MAXLEN, stdin);
	while (nao_fim && buffer[0] == '#')
		nao_fim = fgets(buffer, MAXLEN, stdin);

	sscanf(buffer, "%lf", tol);

	nao_fim = fgets(buffer, MAXLEN, stdin);
	while (nao_fim && buffer[0] == '#')
		nao_fim = fgets(buffer, MAXLEN, stdin);

	sscanf(buffer, "%d", maxeval);

	arredondar_valores(x);
}

int main(void)
{
	double x[N];    /* vari?veis do problema */

	double lb[N];   /* lower bounds para x */
	double ub[N];   /* upper bounds para x */

	double c1data[2]; /* limites lower e upper das restricoes*/

	/* valor de parada da fun??o objetivo, que  para a otimiza??o quando
	 * f(x) >= stopval */
	double stopval;

	/* toler?ncia das restri??es: a otimiza??o para quando fc(x) <= tol */
	double tol;

	int maxeval;    /* quantidade m?xima de avalia??es da fun??o */

	nlopt_opt opt;  /* "objeto" de controle da minimiza??o */
	double maxf;	/* valor da fun??o maximizado pelo NLopt */



	int status;     /* c?digo de retorno da otimiza??o */
	int i;

	struct timeval inicio, fim;


	ler_entrada(x, lb, ub, c1data, &stopval, &tol, &maxeval);

	/* algoritmo COBYLA,Constrained Optimization BY Linear Approximations */
	opt = nlopt_create(NLOPT_LN_COBYLA, N);
	nlopt_set_lower_bounds(opt, lb);
	nlopt_set_upper_bounds(opt, ub);
	nlopt_set_max_objective(opt, objetivo, NULL);

	/* restricao io.sort.mb */
	nlopt_add_inequality_constraint(opt, restricao_io_sort_mb, &c1data[0], tol);
	
	/* restricao io.sort.factor */
	nlopt_add_inequality_constraint(opt, restricao_io_sort_factor, &c1data[2], tol);

	nlopt_set_stopval(opt, stopval);
	nlopt_set_maxeval(opt, maxeval);

	evals = 0;
	gettimeofday(&inicio, NULL);

	status = nlopt_optimize(opt, x, &maxf);

	gettimeofday(&fim, NULL);

	if (status < 0) {
		fprintf(stderr, "nlopt falhou, codigo de erro: %d\n", status);
	} else {
		printf("M?ximo encontrado = %g apos %d avaliacoes, "
				"status = %d\n\nx =\n", maxf, evals, status);

		arredondar_valores(x);
		for (i = 0; i < N; i++)
			printf("%15g   %s\n", x[i], x_names[i]);
		printf("Elapsed time: %ld\n", (long)(fim.tv_sec - inicio.tv_sec));
		//printf("Elapsed time: %fs\n", toseconds(fim) - toseconds(inicio));
	}

	nlopt_destroy(opt);

	return 0;
}
